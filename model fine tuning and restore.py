# -*- coding: utf-8 -*-
"""json_fine_tuing_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m0XZSH7voVsOwiaF_nhjiAvhAUTue365
"""

!pip install transformers torch



def convert(text, indices, vals, puns):
    # Ensure the original text is not modified by creating a new list
    modified_text = text

    # Iterate over the given indices and values
    for val, i in zip(vals, indices):
        # Insert the corresponding pun at the specified index in the modified text
        modified_text.insert(val, puns[i - 1])

    # Return the modified text
    return modified_text

"""
indices = [value for index, value in enumerate(json_objects[0]['labels']) if value > 0]
val = [index for index, value in enumerate(json_objects[0]['labels']) if value > 0]
x=convert(json_objects[0]['text'] ,  indices  , val, puns )
val , indices
"""









# requirments liberiries
from transformers import RobertaTokenizer, RobertaForMaskedLM
from transformers import DataCollatorForLanguageModeling
#masked language modeling (MLM)
tokenizer = RobertaTokenizer.from_pretrained('gerulata/slovakbert')
model = RobertaForMaskedLM.from_pretrained('gerulata/slovakbert')

# Import the torch library for PyTorch functionalities
import torch

# Import the Natural Language Toolkit (nltk) for natural language processing tasks
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Import the regular expression module for text manipulation
import re

# Download the 'punkt' resource from nltk, which includes pre-trained tokenizers
nltk.download('punkt')

# Import necessary libraries and modules
import torch
from transformers import DataCollatorForLanguageModeling, AdamW
from torch.utils.data import DataLoader
from nltk.tokenize import sent_tokenize
import re

def fine_tuning(texts, model, tokenizer):
    # Check if texts is empty
    if len(texts) == 0:
        return model

    # Preprocess texts to focus on punctuation
    def preprocess_for_punctuation(texts):
        processed_texts = []
        for text in texts:
            # Mask punctuation marks with a special token like '[MASK]'
            text = re.sub(r'[.,?!:-]', '[MASK]', text)
            processed_texts.append(text)
        return processed_texts

    # Apply preprocessing to the input texts
    texts = preprocess_for_punctuation(texts)

    # Tokenize and encode the processed texts
    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=512)

    # Define a custom dataset for Masked Language Modeling (MLM)
    class MLM_Dataset(torch.utils.data.Dataset):
        def __init__(self, encodings):
            self.encodings = encodings

        def __len__(self):
            return len(self.encodings['input_ids'])

        def __getitem__(self, idx):
            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    # Create an instance of the custom dataset
    dataset = MLM_Dataset(encodings)

    # Create a data collator for MLM
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

    # Create a DataLoader for training the model
    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=data_collator)

    # Define the optimizer for model training
    optimizer = AdamW(model.parameters(), lr=5e-5)

    # Set the number of training epochs
    epochs = 1

    print("Starting fine-tuning for punctuation...")
    # Training loop
    for epoch in range(epochs):
        model.train()
        for batch in dataloader:
            # Zero the gradients before backward pass
            optimizer.zero_grad()
            # Move inputs to the appropriate device (CPU or GPU)
            inputs = {k: v.to(model.device) for k, v in batch.items()}
            # Forward pass and compute loss
            outputs = model(**inputs)
            loss = outputs.loss
            # Backward pass and optimization step
            loss.backward()
            optimizer.step()

    print("Fine-tuning done.")

    # Return the fine-tuned model
    return model



# Function to restore punctuation in a given text using a language model
def restore_pun(text, model):
    # Tokenize the input text into words using nltk
    words = nltk.word_tokenize(text)

    # Iterate over each word in the tokenized text
    for i in range(1, len(words)):
        current = words[i]

        # Check if the current word is a punctuation mark or not
        if current not in [".", ",", "?", "!" ,":","-"]:
            words[i] += " <mask>"
            current_pun = "no"
        else:
            current_pun = current
            words[i] = " <mask>"

        # Concatenate the words back into a string
        x = " ".join(words)

        # Encode the input using the provided tokenizer
        encoded_input = tokenizer(x, return_tensors='pt')

        # Pass the encoded input through the language model
        output = model(**encoded_input)

        # Find the index of the masked token in the input
        mask_token_index = torch.where(encoded_input["input_ids"][0] == tokenizer.mask_token_id)[0]

        # Extract the logits for the masked token
        mask_token_logits = output.logits[0, mask_token_index, :]

        # Find the token with the highest probability (predicted masked word)
        predicted_token_id = torch.argmax(mask_token_logits).item()
        predicted_token = tokenizer.decode([predicted_token_id])

        # Update the word based on the predicted token and the original punctuation
        if current_pun == "no" and predicted_token in ['.', ',', '?' , '!',':' ,'-' ]:
            words[i] = current + predicted_token
        elif current_pun != "no" and predicted_token in ['.', ',', '?' , '!',':' ,'-' ]:
            words[i] = predicted_token
        else:
            words[i] = current

    # Join the words back into a string and return the result
    out = " ".join(words)
    return out

# Example usage:
# text_to_restore = "This is an example sentence without punctuation marks"
# restored_text = restore_pun(text_to_restore, your_language_model)
# print(restored_text)

# Infinite loop to present a menu and perform actions based on user input
while True:
    # Displaying the menu options and taking user input
    option = input('Choose 1-train the network 2-correct the punctuation 3-exit ')

    # Option 1: Train the network
    if option == '1':
        # Input the path of the data file
        file_path = input('Input the path of your data ')

        # Import necessary libraries and modules
        import json

        # Read and parse each line of the file as a separate JSON object
        json_objects = []
        with open(file_path, 'r') as file:
            for line in file:
                try:
                    json_object = json.loads(line)
                    json_objects.append(json_object)
                except json.JSONDecodeError:
                    # Handle possible decoding errors (e.g., empty lines)
                    continue

        # Define punctuation marks to consider during training
        puns = ['.', ',', '?', '!', ':', '-']

        # Process each JSON object and perform training
        texts = []
        for i in range(len(json_objects)):
            # Extract indices and values from the 'labels' field of the JSON object
            indices = [value for index, value in enumerate(json_objects[i]['labels']) if value > 0]
            val = [index for index, value in enumerate(json_objects[0]['labels']) if value > 0]

            # Apply the 'convert' function to modify the text based on indices, values, and puns
            json_objects[i]['text'] = convert(json_objects[i]['text'], indices, val, puns)

            # Append the modified text to the 'texts' list
            texts.append(" ".join(json_objects[i]['text']))

        # Fine-tune the model using the modified texts
        model = fine_tuning(texts[:], model, tokenizer)

    # Option 2: Correct the punctuation in a user-entered text
    elif option == '2':
        # Input the text for punctuation correction
        test = input('Enter your text: ')

        # Display the corrected text using the 'restore_pun' function
        print("Output:", restore_pun(test, model))

    # Option 3: Exit the loop
    else:
        break

# data for training
text="Áno, Roman, som galakticky naivný, že v politike hľadám dobro, morálku, integritu.."

model =fine_tuning(text ,model , tokenizer)

#input data
test="ako sa voláš"
print("input : "  , test)
print ("output :" ,restore_pun(test ,model))